{
  "missions": [
    {
      "id": 1,
      "title": "Dockerfile Jigsaw",
      "description": "Learn the basic pieces of a Dockerfile by arranging steps in the right order",
      "intro": {
        "what": "Docker is software that packages your code + everything it needs to run into a 'container' - like a sealed lunchbox",
        "why": "Stops 'it works on my machine' problems - everyone runs the same exact environment",
        "where": "A file called `Dockerfile` in your project root (next to your Python code)",
        "how": "Write a Dockerfile → run `docker build` → get an image → run `docker run` → app starts identically on any computer",
        "purpose": "By the end, you’ll recognize the core Dockerfile steps developers use daily (choose a base, install deps, copy code, and start the app)."
      },
      "difficulty": {
        "beginner": { "hints": 5, "blocks": 8 },
        "intermediate": { "hints": 3, "blocks": 10 },
        "advanced": { "hints": 1, "blocks": 12 }
      },
      "teaching": {
      "tldr": "Dockerfiles are recipes that describe how to run your app anywhere, the same way",
        "explainAgain": "Think of it like a recipe: first you get ingredients (FROM), then set up workspace (WORKDIR), copy files (COPY), install dependencies (RUN), and start cooking (CMD)",
        "example": "FROM python:3.12-slim\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\nCOPY . .\nCMD [\"python\", \"app.py\"]",
        "exercise": "Try reordering these instructions to optimize build speed and image size",
        "cheatSheet": "Common directives: FROM (base image), WORKDIR (working directory), COPY (copy files), RUN (execute commands), EXPOSE (port), CMD (start command)"
      },
      "realWorld": {
        "scenarios": [
          {
            "situation": "Your teammate says 'it works on my machine' but fails in staging",
            "solution": "Dockerfiles ensure everyone runs identical environments",
            "dailyUse": "Morning standup: you commit a Dockerfile so the testing team tests the exact same Python version you dev with"
          },
          {
            "situation": "You need to onboard a new developer to a FastAPI project",
            "solution": "They run 'docker build' and have a working environment in 2 minutes",
            "dailyUse": "No more 'install Python 3.12, pip install this, oh wait you need libpq-dev...'"
          }
        ]
      },
      "validation": {
      "requiredOrder": ["FROM", "WORKDIR", "COPY requirements.txt", "RUN pip install", "COPY .", "EXPOSE", "CMD"],
        "blocks": [
          "FROM python:3.12-slim",
          "WORKDIR /app",
          "COPY requirements.txt .",
          "RUN pip install --no-cache-dir -r requirements.txt",
          "COPY . .",
          "EXPOSE 8000",
          "CMD [\"python\", \"app.py\"]",
          "RUN apt-get update",
          "RUN apt-get install -y curl",
          "ENV PYTHONPATH=/app",
          "USER app",
          "HEALTHCHECK CMD curl -f http://localhost:8000/health"
        ]
      },
      "quiz": [
        {
          "question": "What does the FROM directive do in a Dockerfile?",
          "options": [
            "Starts the application",
            "Specifies the base image to build from",
            "Copies files into the container",
            "Opens network ports"
          ],
          "correctAnswer": 1,
          "explanation": "FROM specifies the base image - like choosing what operating system and tools your container starts with. It's always the first line.",
          "wrongAnswerExplanations": {
            "0": "CMD starts the application, not FROM. FROM comes first and sets up the foundation.",
            "2": "COPY copies files. FROM is different - it picks the base image.",
            "3": "EXPOSE opens ports. FROM is the foundation that comes before everything else."
          }
        },
        {
          "question": "What's the correct order for a basic Dockerfile?",
          "options": [
            "RUN → COPY → FROM → CMD",
            "FROM → WORKDIR → COPY → RUN → CMD",
            "CMD → EXPOSE → COPY → FROM",
            "FROM → CMD → COPY → RUN"
          ],
          "correctAnswer": 1,
          "explanation": "Start with FROM (base image), set WORKDIR (where files go), COPY files, RUN commands to install/configure, then CMD (start command).",
          "wrongAnswerExplanations": {
            "0": "FROM must come first - you need a base image before doing anything else.",
            "2": "CMD comes last because it starts the app. First you need FROM, then setup steps.",
            "3": "You need to COPY files and RUN setup commands before starting the app with CMD."
          }
        },
        {
          "question": "What does CMD do?",
          "options": [
            "Runs commands during build time",
            "Starts the application when the container runs",
            "Installs dependencies",
            "Copies files into the container"
          ],
          "correctAnswer": 1,
          "explanation": "CMD defines the command that runs when the container starts. It's what makes your app actually run.",
          "wrongAnswerExplanations": {
            "0": "RUN runs commands during build. CMD runs when the container starts.",
            "2": "RUN pip install installs dependencies. CMD starts your app.",
            "3": "COPY copies files. CMD runs the command to start your application."
          }
        }
      ]
    },
    {
      "id": 2,
      "title": "Cache Crash",
      "description": "Make builds faster and images smaller by reordering a Dockerfile like a pro",
      "intro": {
        "what": "Each Dockerfile instruction creates a 'layer' - Docker reuses unchanged layers to speed up builds",
        "why": "Bad order means slow builds and wasted time/money. Good order makes builds snappy.",
        "where": "In your Dockerfile instruction order and `.dockerignore` file",
        "how": "Put stuff that changes rarely (dependencies) before stuff that changes often (your code)",
        "purpose": "You'll learn practical habits to speed up your local builds and CI runs without needing ops knowledge."
      },
      "difficulty": {
        "beginner": { "hints": 5, "targetSize": 400, "targetTime": 30 },
        "intermediate": { "hints": 3, "targetSize": 300, "targetTime": 25 },
        "advanced": { "hints": 1, "targetSize": 200, "targetTime": 20 }
      },
      "teaching": {
      "tldr": "Docker caches steps. Order your steps to avoid re-doing work and speed up builds",
        "explainAgain": "Like stacking boxes: put heavy, stable boxes (dependencies) at the bottom, light changing boxes (your code) on top. **Caching** means Docker remembers the result of each layer (like a step in your Dockerfile). If nothing changed in that layer, Docker reuses the saved result instead of rebuilding it - this makes builds super fast! **Dependencies** are external packages your code needs to run (like libraries from pip, npm, etc.). They're listed in requirements.txt and usually don't change often, so putting them in layers before your code means those layers get cached and reused.",
        "example": "Copy requirements.txt first, install dependencies, THEN copy your code. This way code changes don't invalidate dependency cache",
        "exercise": "Reorder instructions and toggle optimization flags to hit size and time targets",
        "cheatSheet": "Optimization tips: Use slim base images, copy dependencies before code, use --no-cache-dir, combine RUN commands, use .dockerignore"
      },
      "realWorld": {
        "scenarios": [
          {
            "situation": "You change one line of code and rebuild takes 5 minutes",
            "solution": "Layers weren't optimized - dependencies rebuild every time",
            "dailyUse": "Fix layer order: COPY requirements.txt → RUN pip install → COPY . . Now code changes are instant rebuilds"
          },
          {
            "situation": "CI bill is $200/month",
            "solution": "Fixing cache strategy drops it to $40",
            "dailyUse": "Optimized Dockerfiles = faster builds = less CI minutes = lower bills"
          }
        ]
      },
      "validation": {
        "instructions": [
          "FROM python:3.12-slim",
          "WORKDIR /app",
          "COPY requirements.txt .",
          "RUN pip install --no-cache-dir -r requirements.txt",
          "COPY . .",
          "RUN python -m compileall .",
          "EXPOSE 8000",
          "CMD [\"python\", \"app.py\"]"
        ],
        "optimizations": [
          "--no-cache-dir",
          "--no-install-recommends",
          "multi-stage build",
          ".dockerignore"
        ]
      },
      "quiz": [
        {
          "question": "Why should you copy requirements.txt BEFORE copying your code?",
          "options": [
            "It's faster",
            "Docker caches layers - if requirements.txt doesn't change, pip install won't rerun",
            "It's required by Docker",
            "It makes the image smaller"
          ],
          "correctAnswer": 1,
          "explanation": "**Caching** is Docker's way of remembering what each layer built. When Docker builds your image, it creates a 'layer' for each instruction (like COPY or RUN). If the files that instruction uses haven't changed since the last build, Docker reuses the saved result from that layer instead of rebuilding it. This is called 'layer caching' and it's why copying requirements.txt first helps - if your code changes but requirements.txt doesn't, Docker can skip reinstalling all your dependencies and just reuse the cached layer with dependencies already installed. This saves time!",
          "wrongAnswerExplanations": {
            "0": "It's not just faster - it's about Docker's layer caching system that avoids unnecessary work.",
            "2": "It's not required - it's a best practice to optimize build speed through caching.",
            "3": "Size stays the same, but build time is much faster because dependencies don't reinstall on every code change."
          }
        },
        {
          "question": "What happens if you copy your code BEFORE installing dependencies?",
          "options": [
            "Nothing, it works the same",
            "Every code change invalidates the dependency cache, slowing builds",
            "The image becomes smaller",
            "Docker won't build it"
          ],
          "correctAnswer": 1,
          "explanation": "**Dependencies** are external packages your application needs (like Flask, requests, pandas - listed in requirements.txt). When you copy your code before installing dependencies, every code change invalidates (breaks) the cache for that COPY layer. Since Docker builds layers in order, this means Docker has to rebuild everything that comes after - including reinstalling all your dependencies. This is wasteful because dependencies change rarely, but your code changes often. By copying dependencies first, you ensure that code changes only affect the code layer, not the dependency installation layer.",
          "wrongAnswerExplanations": {
            "0": "It does work, but it's much slower because dependencies rebuild on every code change.",
            "2": "Image size stays the same, but build time increases significantly.",
            "3": "Docker will build it, but you're missing out on optimization benefits."
          }
        },
        {
          "question": "What does --no-cache-dir do in pip install?",
          "options": [
            "Prevents Docker from caching",
            "Skips pip's cache directory to reduce image size",
            "Installs faster",
            "Forces reinstall of packages"
          ],
          "correctAnswer": 1,
          "explanation": "--no-cache-dir tells pip not to keep a cache directory, which reduces the final image size since pip's cache isn't needed in the container.",
          "wrongAnswerExplanations": {
            "0": "It prevents pip from caching, not Docker. Docker layer caching still works.",
            "2": "It doesn't speed up installation - it actually makes pip slightly slower but reduces image size.",
            "3": "It doesn't force reinstall - it just skips pip's cache directory to save space."
          }
        }
      ]
    },
    {
      "id": 3,
      "title": "Pipeline Architect",
      "description": "Set up an automated flow that runs tests, builds your app, and gets it ready to ship",
      "intro": {
        "what": "CI/CD is Continuous Integration/Deployment - robots that test & deploy your code automatically when you push to GitHub",
        "why": "It catches bugs early, keeps main stable, and saves you from manual, error-prone steps",
        "where": "`.github/workflows/` folder (GitHub Actions), `.gitlab-ci.yml` (GitLab), or cloud platforms",
        "how": "Push code → pipeline runs tests → builds Docker image → deploys to staging → you merge to prod",
        "purpose": "See how common pipeline steps fit together so your code is tested and build-ready on every push."
      },
      "difficulty": {
        "beginner": { "hints": 5, "nodes": 4 },
        "intermediate": { "hints": 3, "nodes": 5 },
        "advanced": { "hints": 1, "nodes": 6 }
      },
      "teaching": {
      "tldr": "Pipelines automate test → build → (optional) deploy so your code is always ready",
        "explainAgain": "Like an assembly line: code goes in → tests run → if tests pass → build image → deploy to staging → manual approval → deploy to prod",
        "example": "GitHub Actions workflow: on push → checkout code → setup Python → install deps → run tests → build Docker → push to registry",
        "exercise": "Connect job nodes in the right order and configure each with proper YAML",
        "cheatSheet": "Pipeline stages: Test → Lint → Build → Push → Deploy. Use artifacts to pass data between jobs, secrets for sensitive data"
      },
      "realWorld": {
        "scenarios": [
          {
            "situation": "You push code at 5pm",
            "solution": "Pipeline auto-runs tests, builds Docker image, deploys to staging",
            "dailyUse": "Wake up next day to see your feature already tested and deployed to staging for PM review"
          },
          {
            "situation": "Someone breaks main branch",
            "solution": "Pipeline catches it before customers see it",
            "dailyUse": "Red X in GitHub → click to see which test failed → fix → push → green checkmark"
          }
        ]
      },
      "validation": {
        "requiredJobs": ["test", "lint", "build", "push", "deploy"],
        "yamlTemplate": {
          "test": "# Job name shows in the Actions UI\nname: Test\n# Runner machine where the job executes\nruns-on: ubuntu-latest\n# Steps are executed sequentially in this job\nsteps:\n  # Check out your repository code\n  - uses: actions/checkout@v4\n  # Install the requested Python version\n  - uses: actions/setup-python@v5\n  # Install dependencies listed in requirements.txt\n  - run: pip install -r requirements.txt\n  # Run the test suite with pytest\n  - run: pytest",
          "lint": "# Lint job to enforce code style\nname: Lint\n# Use the latest Ubuntu runner\nruns-on: ubuntu-latest\n# Sequential steps\nsteps:\n  # Get repository code\n  - uses: actions/checkout@v4\n  # Provision Python for flake8\n  - uses: actions/setup-python@v5\n  # Install the linter\n  - run: pip install flake8\n  # Lint the whole repository\n  - run: flake8 .",
          "build": "# Build job builds a Docker image\nname: Build\n# Do not start until test and lint are successful\nneeds: [test, lint]\n# Runner OS\nruns-on: ubuntu-latest\n# Steps to produce the image\nsteps:\n  # Check out source code for the Docker build context\n  - uses: actions/checkout@v4\n  # Official action to build/push Docker images\n  - uses: docker/build-push-action@v6\n    with:\n      # Build only in this job; pushing happens later\n      push: false\n      # Tag image with the commit SHA\n      tags: myapp:${{ github.sha }}",
          "push": "# Push job publishes the image to registry\nname: Push\n# Must run after Build job\nneeds: build\n# Runner machine\nruns-on: ubuntu-latest\n# Steps\nsteps:\n  # Re-use build action to push the image\n  - uses: docker/build-push-action@v6\n    with:\n      # Push to the registry in this job\n      push: true\n      # Same tag used in Build job\n      tags: myapp:${{ github.sha }}",
          "deploy": "# Deploy job simulates deployment to staging\nname: Deploy\n# Deploy after image is pushed\nneeds: push\n# Runner\nruns-on: ubuntu-latest\n# Steps\nsteps:\n  # Replace with real deployment script in production\n  - run: echo 'Deploying to staging'"
        }
      },
      "quiz": [
        {
          "question": "What does the 'needs' keyword do in a GitHub Actions job?",
          "options": [
            "It's optional and does nothing",
            "It makes the job wait for other jobs to finish successfully first",
            "It runs jobs in parallel",
            "It skips the job if others fail"
          ],
          "correctAnswer": 1,
          "explanation": "needs defines dependencies. If Job B needs Job A, Job B waits for Job A to complete successfully before starting.",
          "wrongAnswerExplanations": {
            "0": "needs is important - it controls the order jobs run in. Without it, jobs run in parallel.",
            "2": "Without needs, jobs run in parallel. needs makes them sequential.",
            "3": "If a needed job fails, the dependent job is skipped, but that's a side effect, not the purpose."
          }
        },
        {
          "question": "What's the typical order of a CI/CD pipeline?",
          "options": [
            "Deploy → Build → Test",
            "Test → Build → Push → Deploy",
            "Build → Deploy → Test",
            "Push → Test → Build"
          ],
          "correctAnswer": 1,
          "explanation": "First test your code, then build the image if tests pass, push it to a registry, then deploy. This catches problems early.",
          "wrongAnswerExplanations": {
            "0": "You need to test before deploying! Testing should happen first.",
            "2": "Testing must come before building and deploying to catch bugs early.",
            "3": "You need to test first, then build, then push, then deploy."
          }
        },
        {
          "question": "What does 'runs-on: ubuntu-latest' specify?",
          "options": [
            "The Docker image to use",
            "The operating system of the runner machine",
            "The Python version",
            "Where to deploy"
          ],
          "correctAnswer": 1,
          "explanation": "runs-on specifies the runner environment - the operating system where your job will execute. ubuntu-latest means the latest Ubuntu Linux.",
          "wrongAnswerExplanations": {
            "0": "The Docker image is specified in your Dockerfile. runs-on is about the runner machine.",
            "2": "Python version is set with actions/setup-python, not runs-on.",
            "3": "Deployment target is configured separately. runs-on is about where the job runs."
          }
        }
      ]
    },
    {
      "id": 4,
      "title": "Log Detective",
      "description": "Practice reading CI logs so you can quickly find and fix build/test failures",
      "intro": {
        "what": "CI logs are text output showing everything your pipeline did - tests, builds, errors",
        "why": "When builds fail, logs tell you what broke and where to look in your code or config",
        "where": "GitHub Actions tab, GitLab CI/CD section, or your CI platform dashboard",
        "how": "Pipeline fails → open logs → search for 'Error' or red lines → fix the issue → push again",
        "purpose": "Build confidence reading noisy logs and extracting the one or two messages that matter."
      },
      "difficulty": {
        "beginner": { "hints": 5, "errors": 3 },
        "intermediate": { "hints": 3, "errors": 4 },
        "advanced": { "hints": 1, "errors": 5 }
      },
      "teaching": {
      "tldr": "CI logs are your debugging guide—they show what went wrong and where to look",
        "explainAgain": "Like a detective story: follow the clues (error messages) to find the culprit (buggy code)",
        "example": "ModuleNotFoundError: No module named 'requests' → check requirements.txt → add missing dependency → push fix",
        "exercise": "Read through simulated CI logs and identify all the errors causing the pipeline to fail",
        "cheatSheet": "Common CI errors: ModuleNotFoundError (missing deps), FileNotFoundError (wrong paths), syntax errors, secret not found, wrong Python version"
      },
      "realWorld": {
        "scenarios": [
          {
            "situation": "Production deploy fails at 2am",
            "solution": "You SSH nowhere, just read GitHub Actions logs",
            "dailyUse": "Open phone → GitHub app → Actions tab → see red X → click logs → find error → fix → push → back to sleep"
          },
          {
            "situation": "New junior dev asks 'why is pytest failing in CI but not locally?'",
            "solution": "You show them the log's Python version mismatch",
            "dailyUse": "CI uses Python 3.11, local uses 3.12 → update CI to match local → tests pass"
          }
        ]
      },
      "validation": {
        "logLines": [
          "::group::Run pytest",
          "============================= test session starts ==============================",
          "platform linux -- Python 3.11.0, pytest-7.4.0, pluggy-1.0.0",
          "rootdir: /home/runner/work/myapp/myapp",
          "collected 15 items",
          "",
          "test_app.py::test_health_check PASSED [  6%]",
          "test_app.py::test_get_users FAILED [ 12%]",
          "",
          "=================================== FAILURES ====================================",
          "________________________ test_get_users _________________________",
          "",
          "    def test_get_users():",
          ">       response = requests.get('http://localhost:8000/users')",
          "E       ModuleNotFoundError: No module named 'requests'",
          "",
          "test_app.py:5: ModuleNotFoundError",
          "============================== short test summary info ==============================",
          "FAILED test_app.py::test_get_users - ModuleNotFoundError: No module named 'requests'",
          "============================== 1 failed, 14 passed in 2.34s ==============================",
          "::endgroup::",
          "",
          "::group::Run docker build",
          "Sending build context to Docker daemon  2.048kB",
          "Step 1/6 : FROM python:3.12-slim",
          " ---> abc123def456",
          "Step 2/6 : WORKDIR /app",
          " ---> Running in def456ghi789",
          " ---> Removed intermediate container def456ghi789",
          "Step 3/6 : COPY requirements.txt .",
          " ---> Using cache",
          "Step 4/6 : RUN pip install -r requirements.txt",
          " ---> Running in ghi789jkl012",
          "ERROR: Could not find a version that satisfies the requirement fastapi==99.9.9",
          "ERROR: No matching distribution found for fastapi==99.9.9",
          "The command '/bin/sh -c pip install -r requirements.txt' returned a non-zero code: 1",
          "::endgroup::"
        ],
        "errors": [
          "ModuleNotFoundError: No module named 'requests'",
          "ERROR: Could not find a version that satisfies the requirement fastapi==99.9.9",
          "FileNotFoundError: [Errno 2] No such file or directory: 'requirements.txt'"
        ]
      },
      "quiz": [
        {
          "question": "What does 'ModuleNotFoundError: No module named X' mean?",
          "options": [
            "The file doesn't exist",
            "A Python package is missing from the environment",
            "The code has a syntax error",
            "The test is wrong"
          ],
          "correctAnswer": 1,
          "explanation": "ModuleNotFoundError means Python can't find a package that your code is trying to import. Usually fixed by adding it to requirements.txt.",
          "wrongAnswerExplanations": {
            "0": "FileNotFoundError means a file doesn't exist. ModuleNotFoundError is about missing Python packages.",
            "2": "Syntax errors show different messages. ModuleNotFoundError is specifically about missing imports.",
            "3": "The test might be fine - the problem is that the required package isn't installed."
          }
        },
        {
          "question": "Where should you look first when CI logs show an error?",
          "options": [
            "Scroll to the top of the logs",
            "Look for lines containing 'ERROR', 'FAILED', or 'Exception'",
            "Check the deployment section",
            "Ignore the logs and check your code"
          ],
          "correctAnswer": 1,
          "explanation": "CI logs can be long, but error messages usually contain keywords like ERROR, FAILED, or Exception. Use Ctrl+F to search for these.",
          "wrongAnswerExplanations": {
            "0": "Errors usually appear near the end where the failure happened, not at the top.",
            "2": "Deployment happens after build/test. Check build/test errors first.",
            "3": "Logs tell you exactly what went wrong - they're your best debugging tool!"
          }
        },
        {
          "question": "If CI fails with 'ERROR: Could not find a version that satisfies the requirement', what's likely wrong?",
          "options": [
            "The package name is misspelled or version doesn't exist",
            "Python version is wrong",
            "The internet is down",
            "The test is failing"
          ],
          "correctAnswer": 0,
          "explanation": "This error usually means the package name is wrong, the version specified doesn't exist, or there's a typo in requirements.txt.",
          "wrongAnswerExplanations": {
            "1": "Wrong Python version would show a different error. This is about package availability.",
            "2": "If internet was down, you'd see a connection error, not a version error.",
            "3": "This is a dependency installation error, not a test failure."
          }
        }
      ]
    },
    {
      "id": 5,
      "title": "Deploy or Die",
      "description": "Fill in a few key settings so your built app can run in the cloud",
      "intro": {
        "what": "Deployment is taking your built Docker image and running it on a server (cloud) where users can access it",
        "why": "You need a couple of basics: where to pull the image, secrets, and env settings",
        "where": "Cloud platforms: Azure, AWS, Render, Heroku, Docker Hub, or simple hosting services",
        "how": "Merge PR → pipeline builds image → tags it → pushes to registry → cloud pulls & runs it",
        "purpose": "Understand the minimum info a developer usually provides so a simple app can go live."
      },
      "difficulty": {
        "beginner": { "hints": 5, "fields": 6 },
        "intermediate": { "hints": 3, "fields": 8 },
        "advanced": { "hints": 1, "fields": 10 }
      },
      "teaching": {
      "tldr": "Deployment is getting your containerized app running on a server for users",
        "explainAgain": "Like moving into a new house: you need the right address (registry), keys (secrets), utilities (environment), and insurance (rollback plan)",
        "example": "Docker Hub registry → tag image with version → push → cloud platform pulls image → runs with environment variables → app is live",
        "exercise": "Fill in deployment configuration forms with correct values for registry, secrets, and environment settings",
        "cheatSheet": "Deployment checklist: Registry URL, image tags, environment variables, secrets, health checks, rollback strategy"
      },
      "realWorld": {
        "scenarios": [
          {
            "situation": "Feature complete → you merge PR",
            "solution": "Pipeline auto-deploys to staging for PM review",
            "dailyUse": "PM gets Slack notification with staging URL → tests feature → approves → you merge to main → auto-deploys to prod"
          },
          {
            "situation": "Need to rollback bad deploy",
            "solution": "You revert commit and pipeline redeploys last good version",
            "dailyUse": "GitHub → revert commit → pipeline triggers → deploys previous working version → customers happy again"
          }
        ]
      },
      "validation": {
        "requiredFields": [
          "registry_url",
          "image_tag",
          "environment",
          "database_url",
          "api_key",
          "deployment_strategy"
        ],
        "validValues": {
          "registry_url": ["docker.io/myapp", "ghcr.io/myorg/myapp", "myregistry.com/myapp"],
          "image_tag": ["latest", "v1.0.0", "main-abc123", "staging-def456"],
          "environment": ["production", "staging", "development"],
          "deployment_strategy": ["rolling", "blue-green", "canary"]
        }
      },
      "quiz": [
        {
          "question": "What is a container registry?",
          "options": [
            "A place to store Docker images",
            "A deployment platform",
            "A CI/CD tool",
            "A database"
          ],
          "correctAnswer": 0,
          "explanation": "A registry (like Docker Hub, GitHub Container Registry) stores Docker images so they can be pulled and run on servers.",
          "wrongAnswerExplanations": {
            "1": "Registries store images, but deployment platforms (like AWS, Azure) pull from registries to run them.",
            "2": "CI/CD builds and pushes images to registries, but registries themselves just store images.",
            "3": "Databases store data. Registries store container images."
          }
        },
        {
          "question": "Why do you need environment variables in deployment?",
          "options": [
            "They're not needed",
            "They configure the app for different environments (dev/staging/prod)",
            "They make the app faster",
            "They're just for debugging"
          ],
          "correctAnswer": 1,
          "explanation": "Environment variables let the same image run differently in dev vs staging vs production (different database URLs, API keys, etc).",
          "wrongAnswerExplanations": {
            "0": "Environment variables are essential - they configure how your app connects to databases, APIs, etc.",
            "2": "They don't affect speed - they configure behavior and connections.",
            "3": "While useful for debugging, they're primarily for configuring the app per environment."
          }
        },
        {
          "question": "What's the difference between 'latest' and a version tag like 'v1.0.0'?",
          "options": [
            "No difference",
            "'latest' always points to the newest, version tags are fixed to a specific release",
            "Version tags are faster",
            "'latest' is for production only"
          ],
          "correctAnswer": 1,
          "explanation": "'latest' is a moving target - it changes. Version tags like 'v1.0.0' pin to a specific build, which is safer for production.",
          "wrongAnswerExplanations": {
            "0": "They're very different - 'latest' updates automatically, versions are fixed.",
            "2": "Performance is the same. It's about predictability and rollback safety.",
            "3": "'latest' is actually riskier for production - use version tags for stability."
          }
        }
      ]
    },
    {
      "id": 6,
      "title": "Outage Simulator",
      "description": "Learn a simple playbook for what to do when production breaks",
      "intro": {
        "what": "Incident response is what you do when production breaks - diagnose, fix, and restore service FAST",
        "why": "Bugs happen. What matters is restoring service safely and quickly",
        "where": "Your CI pipeline dashboard, team Slack alerts, and basic health checks",
        "how": "Get alert → check health → choose rollback, hotfix, or redeploy based on risk",
        "purpose": "Pick a safe, developer-friendly response when things go wrong, without needing ops expertise."
      },
      "difficulty": {
        "beginner": { "hints": 5, "timeLimit": 300 },
        "intermediate": { "hints": 3, "timeLimit": 240 },
        "advanced": { "hints": 1, "timeLimit": 180 }
      },
      "teaching": {
      "tldr": "Use a simple, repeatable process to restore service safely and quickly",
        "explainAgain": "Like emergency response: assess situation → choose strategy → execute → verify fix → learn from incident",
        "example": "Service down → check logs → see memory leak → choose rollback (safe) → execute → service restored → post-incident review",
        "exercise": "Monitor service health and choose the right response strategy based on the situation",
        "cheatSheet": "Response strategies: Rollback (revert to last known good), Hotfix (quick patch), Redeploy (tested fix), Scale (add resources)"
      },
      "realWorld": {
        "scenarios": [
          {
            "situation": "Pager goes off → service down",
            "solution": "You choose rollback vs hotfix based on severity",
            "dailyUse": "Check Slack → 'Service down' → open CI dashboard → see error spike → rollback to previous version → service restored in 5 minutes"
          },
          {
            "situation": "Customer reports 'app is slow'",
            "solution": "You check metrics, see memory leak, deploy fix",
            "dailyUse": "Check CI dashboard → CPU 100% → check logs → memory leak in new feature → hotfix → deploy → monitor → resolved"
          }
        ]
      },
      "validation": {
        "strategies": [
          {
            "name": "Rollback",
            "description": "Revert to the last known good version",
            "timeToExecute": 5,
            "risk": "low",
            "whenToUse": "When you know the current version is broken"
          },
          {
            "name": "Hotfix",
            "description": "Deploy a quick patch without full testing",
            "timeToExecute": 2,
            "risk": "high",
            "whenToUse": "When you need immediate fix and understand the issue"
          },
          {
            "name": "Redeploy",
            "description": "Deploy a tested fix through normal pipeline",
            "timeToExecute": 8,
            "risk": "medium",
            "whenToUse": "When you have a tested fix ready"
          }
        ],
        "healthThresholds": {
          "critical": 20,
          "warning": 50,
          "healthy": 80
        }
      },
      "quiz": [
        {
          "question": "What should you do first when production is down?",
          "options": [
            "Panic and deploy everything",
            "Check health metrics and logs to understand what broke",
            "Blame someone else",
            "Wait and hope it fixes itself"
          ],
          "correctAnswer": 1,
          "explanation": "Always assess first - check logs, metrics, and health status to understand the problem before taking action.",
          "wrongAnswerExplanations": {
            "0": "Blindly deploying can make things worse. Understand the problem first.",
            "2": "Blaming doesn't fix production. Focus on solving the issue.",
            "3": "Production outages need action. Waiting rarely helps."
          }
        },
        {
          "question": "When should you use Rollback vs Hotfix?",
          "options": [
            "Always use hotfix - it's faster",
            "Rollback when you know current version is broken; hotfix only if you understand the issue",
            "They're the same thing",
            "Always use rollback - it's safer"
          ],
          "correctAnswer": 1,
          "explanation": "Rollback is safest when you know the current version is broken. Hotfix is riskier but faster if you understand the issue.",
          "wrongAnswerExplanations": {
            "0": "Hotfix is risky - only use it if you understand the problem. Rollback is safer.",
            "2": "They're very different - rollback reverts, hotfix patches. Choose based on situation.",
            "3": "Rollback is safer, but if you understand the issue, a hotfix might be faster."
          }
        },
        {
          "question": "What's the goal of incident response?",
          "options": [
            "Find who to blame",
            "Restore service quickly and safely",
            "Deploy the latest code",
            "Make sure everyone knows you're working"
          ],
          "correctAnswer": 1,
          "explanation": "The primary goal is restoring service safely and quickly. Post-incident review (not blame) helps prevent future issues.",
          "wrongAnswerExplanations": {
            "0": "Blaming doesn't help. Focus on fixing the issue.",
            "2": "Deploying latest code might make it worse. Assess first, then act.",
            "3": "Communication is good, but the goal is fixing the problem, not appearances."
          }
        }
      ]
    }
  ]
}
